{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7626105b-ca8e-4b44-8238-619510474e6e",
   "metadata": {},
   "source": [
    "<h1>Deep Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6f816-f6db-4366-8618-45daa07e2e9a",
   "metadata": {},
   "source": [
    "(Small advertising: don't activate the environment from base environment, think to first deactivate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ec1e37-c935-469c-9758-1f2711de1e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theano\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering(dim_ordering=\"th\") #we want to use Theano backend image dim order\n",
    "print(K.backend()) #we use Theano as backend\n",
    "#we can see that Theano is used in /media/anthony/USOPP/conda/IA/etc/conda/activate.d/keras_activate.sh\n",
    "\n",
    "import tensorboard\n",
    "%matplotlib inline\n",
    "np.random.seed(seed=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "186d86ec-f0e9-422a-ae43-87907112f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_PATH = '/media/anthony/USOPP/mnist_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3fb0c-4fc7-47a6-b52b-7ab3e26304e4",
   "metadata": {},
   "source": [
    "MNIST dataset at <a href='http://yann.lecun.com/exdb/mnist/', target='_blank'>http://yann.lecun.com/exdb/mnist/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba278f-ab72-45a6-b5a0-11e0ca5333f7",
   "metadata": {},
   "source": [
    "<h2>2) Convolutionnal Neural Network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4baa4-6ce0-4545-8475-9d7a0b097ecb",
   "metadata": {},
   "source": [
    "<h3>A) MNIST dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3f9fa7c-40a1-446a-a2d9-0cf9b4052381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet:\n",
    "    def __init__(self, input_shape, classes):\n",
    "        self.model = Sequential()\n",
    "        self.input_shape = input_shape\n",
    "        self.classes = classes\n",
    "    \n",
    "    def build(self):\n",
    "        self.model.add(Conv2D(filters=20, kernel_size=(5, 5), padding='same',\\\n",
    "                              input_shape=self.input_shape, activation='relu'))\n",
    "        #self.model.add(Activation(activation=\"relu\"))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "        self.model.add(Conv2D(filters=50, kernel_size=(5, 5), padding='same', activation='relu'))\n",
    "        #self.model.add(Activation(activation=\"relu\"))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(units=500, activation='relu'))\n",
    "        #self.model.add(Activation(activation=\"relu\"))\n",
    "        self.model.add(Dense(units=self.classes, activation='relu'))\n",
    "        #self.model.add(Activation(activation=\"softmax\"))\n",
    "        \n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d5185-ace5-4220-b327-a4885407ace8",
   "metadata": {},
   "source": [
    "<h4>Remarks :</h4>\n",
    "<ul>\n",
    "    <li>The <strong>'filters'</strong> parameter can be the number of convolutionnal filters</li>\n",
    "    <li><strong>Increasing the number of filters in deeper layers is a common technique used in deep learning</strong></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8bf96b2-915d-4ac2-ab52-8b468dbfec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "OPTIMIZER = Adam()\n",
    "VALIDATION_SPLIT = 0.2\n",
    "HEIGHT = WIDTH = 28\n",
    "NB_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99882c6-1581-4382-b061-9432f8907a3b",
   "metadata": {},
   "source": [
    "<h4>Processing :</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4142dbd6-8704-4145-9467-b2d5e4e1b960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(10000, 1, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = loadlocal_mnist(images_path=f'{MNIST_PATH}/train-images-idx3-ubyte',\n",
    "                                   labels_path=f'{MNIST_PATH}/train-labels-idx1-ubyte')\n",
    "\n",
    "X_test, Y_test = loadlocal_mnist(images_path=f'{MNIST_PATH}/t10k-images-idx3-ubyte',\n",
    "                                 labels_path=f'{MNIST_PATH}/t10k-labels-idx1-ubyte')\n",
    "\n",
    "#resize images\n",
    "X_train = np.resize(X_train, new_shape=(X_train.shape[0], HEIGHT, WIDTH))\n",
    "X_test = np.resize(X_test, new_shape=(X_test.shape[0], HEIGHT, WIDTH))\n",
    "#for theano backend we need dim ordered as (batch, channels, width, height)\n",
    "X_train = X_train[:, np.newaxis, :, :]\n",
    "X_test = X_test[:, np.newaxis, :, :]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "#rescale images\n",
    "X_train = X_train.astype('float32') #for rescaling part\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y=Y_train, num_classes=NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y=Y_test, num_classes=NB_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ca231-404e-4fb9-bd83-136e296effb6",
   "metadata": {},
   "source": [
    "<h4>Modelling : </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccaa108f-10e1-4fad-99d7-15cf0f89728c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 20, 28, 28)        520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 14, 14)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 14, 14)        25050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 7, 7)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2450)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               1225500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 1,256,080\n",
      "Trainable params: 1,256,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      " 8448/48000 [====>.........................] - ETA: 1:20 - loss: 5.9289 - acc: 0.3778"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-265196a5a5ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCHS,\n\u001b[0;32m----> 6\u001b[0;31m                     validation_split=VALIDATION_SPLIT)\n\u001b[0m",
      "\u001b[0;32m/media/anthony/USOPP/conda/IA/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/media/anthony/USOPP/conda/IA/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/media/anthony/USOPP/conda/IA/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/anthony/USOPP/conda/IA/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/anthony/USOPP/conda/IA/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Lenet(input_shape=(1, HEIGHT, WIDTH), classes=NB_CLASSES)\n",
    "model = model.build()\n",
    "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCHS,\n",
    "                    validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2464f7-6835-4ea1-b24e-becc09e2d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = model.evaluate(x=X_test, y=Y_test)\n",
    "print(f'Loss function minima for testing set : {test_score[0]:.3f}')\n",
    "print(f'Accuracy maxima for testing set: {test_score[1]:.3f}')\n",
    "\n",
    "train_score = model.evaluate(x=X_train, y=Y_train)\n",
    "print(f'Loss function minima for training set : {train_score[0]:.3f}')\n",
    "print(f'Accuracy maxima for training set: {train_score[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c338e5-4df6-4ae0-b08e-25dba9862ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(15, 5))\n",
    "acc = figure.add_subplot(1, 2, 1)\n",
    "acc.plot(history.history['acc'], c='b', label='train')\n",
    "acc.plot(history.history['val_acc'], c='r', label='test')\n",
    "acc.set_title('Model accuracy')\n",
    "acc.set_xlabel('epochs')\n",
    "acc.set_ylabel('accuracy')\n",
    "acc.legend()\n",
    "\n",
    "loss = figure.add_subplot(1, 2, 2)\n",
    "loss.plot(history.history['loss'], c='b', label='train')\n",
    "loss.plot(history.history['val_loss'], c='r', label='test')\n",
    "loss.set_title('Loss function')\n",
    "loss.set_xlabel('epochs')\n",
    "loss.set_ylabel('loss')\n",
    "loss.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc662c-302f-4b9d-a1b6-5f2a7e165762",
   "metadata": {},
   "source": [
    "<h3>B) CIFAR-10 dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83865ef7-9e59-4e7b-afeb-c48e0789be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CHANNEL = 3 #Because RGB images\n",
    "HEIGHT = WIDTH = 32\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 40\n",
    "NB_CLASSES = 10\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop() #closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573e83a-376c-414c-b9e0-b889c780832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e5698-c80b-476d-9170-2964feeb3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resizing\n",
    "#for theano backend we need dim ordered as (batch, channels, width, height)\n",
    "#X_train = np.moveaxis(a=X_train, source=3, destination=1)\n",
    "#X_test = np.moveaxis(a=X_test, source=3, destination=1)\n",
    "\n",
    "#rescaling\n",
    "X_train = X_train.astype('float')\n",
    "X_train = X_train/255\n",
    "X_test = X_test.astype('float')\n",
    "X_test = X_test/255\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ae483-d871-4891-8913-384c59bde89f",
   "metadata": {},
   "source": [
    "<h4>Modelling :</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140b568-4260-4da3-a8be-4b69f9199aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar_net:\n",
    "    def __init__(self, input_shape, classes):\n",
    "        self.model = Sequential()\n",
    "        self.input_shape = input_shape\n",
    "        self.classes = classes\n",
    "    \n",
    "    def build(self):\n",
    "        self.model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', \n",
    "                              activation='relu', input_shape=self.input_shape))\n",
    "        self.model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', \n",
    "                              activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(rate=0.25))\n",
    "        self.model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', \n",
    "                              activation='relu'))\n",
    "        self.model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', \n",
    "                              activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(rate=0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(units=512, activation='relu'))\n",
    "        self.model.add(Dropout(rate=0.5))\n",
    "        self.model.add(Dense(units=self.classes, activation='softmax'))\n",
    "        \n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566fd86-802c-40b4-bdbe-fce00658cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_model = Cifar_net(input_shape=(IMG_CHANNEL, HEIGHT, WIDTH), classes=NB_CLASSES)\n",
    "cifar_model = cifar_model.build()\n",
    "cifar_model.summary()\n",
    "cifar_model.compile(optimizer=OPTIM, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = cifar_model.fit(X_train, Y_train, batch_size=BATCH_SIZE, \n",
    "                          epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ddc1db-01cb-4706-a22d-431ddd0d3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = cifar_model.evaluate(x=X_test, y=Y_test)\n",
    "print(f'Loss function minima for testing set : {test_score[0]:.3f}')\n",
    "print(f'Accuracy maxima for testing set: {test_score[1]:.3f}')\n",
    "\n",
    "train_score = cifar_model.evaluate(x=X_train, y=Y_train)\n",
    "print(f'Loss function minima for training set : {train_score[0]:.3f}')\n",
    "print(f'Accuracy maxima for training set: {train_score[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72874409-e477-4ba6-8fd0-2b0929d0d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(15, 5))\n",
    "acc = figure.add_subplot(1, 2, 1)\n",
    "acc.plot(history.history['acc'], c='b', label='train')\n",
    "acc.plot(history.history['val_acc'], c='r', label='test')\n",
    "acc.set_title('Model accuracy')\n",
    "acc.set_xlabel('epochs')\n",
    "acc.set_ylabel('accuracy')\n",
    "acc.legend()\n",
    "\n",
    "loss = figure.add_subplot(1, 2, 2)\n",
    "loss.plot(history.history['loss'], c='b', label='train')\n",
    "loss.plot(history.history['val_loss'], c='r', label='test')\n",
    "loss.set_title('Loss function')\n",
    "loss.set_xlabel('epochs')\n",
    "loss.set_ylabel('loss')\n",
    "loss.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
